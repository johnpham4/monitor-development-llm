{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eyJLOT-AsNcI",
        "outputId": "3ea3409b-84ad-40cf-bc51-a3bc62135125"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.8/8.8 MB\u001b[0m \u001b[31m67.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m82.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m51.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.3/139.3 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m74.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.8/147.8 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.9/114.9 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.0/85.0 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.7/85.7 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m752.6/752.6 kB\u001b[0m \u001b[31m40.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m203.4/203.4 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip -q install mlflow transformers boto3"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set up"
      ],
      "metadata": {
        "id": "SZVV105a3LxA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, TextIteratorStreamer\n",
        "import mlflow\n",
        "import boto3\n",
        "from threading import Thread\n",
        "import time"
      ],
      "metadata": {
        "id": "w-BDW6Y1saab"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# mlflow set tracking\n",
        "url = \"https://victoria-communicable-sometimes.ngrok-free.dev\"\n",
        "mlflow.set_tracking_uri(url)\n",
        "tracking_uri = mlflow.get_tracking_uri()\n",
        "print(f\"Current tracking uri: {tracking_uri}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PhwdfPuesdEa",
        "outputId": "54232a43-c256-4157-d741-a79aa5519e48"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current tracking uri: https://victoria-communicable-sometimes.ngrok-free.dev\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mlflow.set_experiment(\"healthcarechatbot\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DuvnGgGPskcJ",
        "outputId": "339aa518-a8ec-4d13-a3b2-fc7ae3fbe5d5"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Experiment: artifact_location='mlflow-artifacts:/1', creation_time=1761320025344, experiment_id='1', last_update_time=1761320025344, lifecycle_stage='active', name='healthcarechatbot', tags={}>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from mlflow.tracking import MlflowClient\n",
        "import re\n",
        "\n",
        "client = MlflowClient()\n",
        "model_name = \"health-llm\"\n",
        "versions = client.get_latest_versions(model_name)\n",
        "\n",
        "for v in versions:\n",
        "    path = v.source\n",
        "    print(\"Model URI:\", path)\n",
        "\n",
        "    match = re.match(r\"s3://([^/]+)/(.*)\", path)\n",
        "    if match:\n",
        "        bucket = match.group(2)\n",
        "        print(\"Bucket:\", bucket)\n",
        "    else:\n",
        "        print(\"Invalid model URI format\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kBjiyz7gslgv",
        "outputId": "5c992c98-fa89-422a-c2e6-54fd0d536509"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2271236965.py:6: FutureWarning: ``mlflow.tracking.client.MlflowClient.get_latest_versions`` is deprecated since 2.9.0. Model registry stages will be removed in a future major release. To learn more about the deprecation of model registry stages, see our migration guide here: https://mlflow.org/docs/latest/model-registry.html#migrating-from-stages\n",
            "  versions = client.get_latest_versions(model_name)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model URI: s3://mlflow-artifacts-monitor/models/health-llm/257cf75b54b449aeb1014c6b480d7bae\n",
            "Bucket: models/health-llm/257cf75b54b449aeb1014c6b480d7bae\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Downloading"
      ],
      "metadata": {
        "id": "L3JPDQ0p3N2t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import boto3\n",
        "from tqdm import tqdm\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "def load_model_from_s3(s3_prefix: str, local_dir: str = \"downloaded_model\"):\n",
        "    \"\"\"\n",
        "    Download an entire model directory (e.g., from MLflow-registered S3 prefix).\n",
        "    Example s3_prefix: \"models/health-llm/b3f91d2b6f42464aab9b9ff07d22ad89\"\n",
        "    \"\"\"\n",
        "\n",
        "    load_dotenv()\n",
        "\n",
        "    aws_access_key = os.getenv(\"AWS_ACCESS_KEY_ID\")\n",
        "    aws_secret_key = os.getenv(\"AWS_SECRET_ACCESS_KEY\")\n",
        "    aws_region = os.getenv(\"AWS_DEFAULT_REGION\", \"ap-southeast-2\")\n",
        "    bucket_name = os.getenv(\"AWS_BUCKET_NAME\", \"mlflow-artifacts-monitor\")\n",
        "\n",
        "    if not all([aws_access_key, aws_secret_key, bucket_name]):\n",
        "        raise ValueError(\"Missing AWS credentials or bucket name in .env file\")\n",
        "\n",
        "    s3 = boto3.client(\n",
        "        \"s3\",\n",
        "        aws_access_key_id=aws_access_key,\n",
        "        aws_secret_access_key=aws_secret_key,\n",
        "        region_name=aws_region\n",
        "    )\n",
        "\n",
        "    os.makedirs(local_dir, exist_ok=True)\n",
        "\n",
        "    paginator = s3.get_paginator(\"list_objects_v2\")\n",
        "    total_files = 0\n",
        "\n",
        "    # Đếm file trước (để tqdm chạy đẹp)\n",
        "    for page in paginator.paginate(Bucket=bucket_name, Prefix=s3_prefix):\n",
        "        for obj in page.get(\"Contents\", []):\n",
        "            total_files += 1\n",
        "\n",
        "    with tqdm(total=total_files, desc=f\"Downloading model from {s3_prefix}\") as pbar:\n",
        "        for page in paginator.paginate(Bucket=bucket_name, Prefix=s3_prefix):\n",
        "            for obj in page.get(\"Contents\", []):\n",
        "                key = obj[\"Key\"]\n",
        "                local_path = os.path.join(local_dir, os.path.relpath(key, s3_prefix))\n",
        "                os.makedirs(os.path.dirname(local_path), exist_ok=True)\n",
        "\n",
        "                s3.download_file(bucket_name, key, local_path)\n",
        "                pbar.update(1)\n",
        "\n",
        "    print(f\"Model downloaded successfully → {local_dir}\")\n",
        "    return local_dir\n"
      ],
      "metadata": {
        "id": "FCiJilD-smWT"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "local_dir = load_model_from_s3(bucket)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-QoEWfuIspbQ",
        "outputId": "95b73b59-5e3c-41e8-aeac-346e4e3e08e4"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading model from models/health-llm/257cf75b54b449aeb1014c6b480d7bae: 100%|██████████| 9/9 [01:03<00:00,  7.09s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model downloaded successfully → downloaded_model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Inference"
      ],
      "metadata": {
        "id": "J2vJ2B9u3RSH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load tokenizer và model từ local\n",
        "tokenizer = AutoTokenizer.from_pretrained(local_dir)\n",
        "model = AutoModelForCausalLM.from_pretrained(local_dir)"
      ],
      "metadata": {
        "id": "IOBZIcwy3UIm"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"Dựa vào Điều 59 của Luật Hàng không dân dụng Việt Nam, chức năng chính của Cảng vụ hàng không là gì?\""
      ],
      "metadata": {
        "id": "aDTT16pw82AM"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Batch predict"
      ],
      "metadata": {
        "id": "0y7_l16e3alW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = [\n",
        "    {\"role\": \"system\", \"content\": \"Bạn là một trợ lý luật pháp Việt Nam thông minh, luôn trả lời bằng tiếng Việt chuẩn và dễ hiểu.\"},\n",
        "    {\"role\": \"user\", \"content\": question}\n",
        "]\n",
        "\n",
        "prompt = tokenizer.apply_chat_template(prompt, tokenize=False, add_generation_prompt=True)\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "start = time.time()\n",
        "outputs = model.generate(\n",
        "    **inputs,\n",
        "    max_new_tokens=384,\n",
        "    do_sample=True,\n",
        "    temperature=0.7,\n",
        "    top_k=50,\n",
        "    top_p=0.9,\n",
        "    repetition_penalty=1.2,\n",
        "    pad_token_id=tokenizer.pad_token_id,\n",
        "    eos_token_id=tokenizer.eos_token_id\n",
        ")\n",
        "durations = time.time() - start\n",
        "\n",
        "print(f\"Time: {durations:.3f} giây\")\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
      ],
      "metadata": {
        "id": "FVD8vJF43W-A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b90f565-a0a0-4bb1-9efe-50cc6638ae39"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time: 109.184 giây\n",
            "\n",
            "Bạn là một trợ lý luật pháp Việt Nam thông minh, luôn trả lời bằng tiếng Việt chuẩn và dễ hiểu.\n",
            " \n",
            " \n",
            "Dựa vào Điều 59 của Luật Hàng không dân dụng Việt Nam, chức năng chính của Cảng vụ hàng không là gì?\n",
            " \n",
            " \n",
            "Chức năng chính của Cảng vụ hàng không là tổ chức đơn vị hành vi và hệ quốc tế, tổ chức hoặc xã sâu người đồng hành quyền trên đất nước, cần có thêm thông tin sau: Thời điểm hán hành và số công viêctao phát sinh ra; thời điểm hán hành và mức độ cho vai trò của họ; nghiệp vụ hành tác nguyên liệu đồng hữu, để an toàn đem lao đặt chúng.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Streaming Predict"
      ],
      "metadata": {
        "id": "bpiNnyax3fDb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "streamer = TextIteratorStreamer(tokenizer, skip_special_tokens=True)\n",
        "\n",
        "generation_kwargs = dict(\n",
        "    **inputs,\n",
        "    max_new_tokens=384,\n",
        "    do_sample=True,\n",
        "    temperature=0.7,\n",
        "    top_k=50,\n",
        "    top_p=0.9,\n",
        "    repetition_penalty=1.2,\n",
        "    pad_token_id=tokenizer.pad_token_id,\n",
        "    eos_token_id=tokenizer.eos_token_id,\n",
        "    streamer=streamer\n",
        ")\n",
        "thread = Thread(target=model.generate, kwargs=generation_kwargs)\n",
        "thread.start()\n",
        "\n",
        "print(\"Assistant:\", end=\" \", flush=True)\n",
        "for new_text in streamer:\n",
        "    print(new_text, end=\"\", flush=True)\n",
        "thread.join()"
      ],
      "metadata": {
        "id": "nr68MMdj3g0z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a03cd1e-cada-4627-c349-cf76d938712e"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Assistant: \n",
            "Bạn là một trợ lý luật pháp Việt Nam thông minh, luôn trả lời bằng tiếng Việt chuẩn và dễ hiểu.\n",
            " \n",
            " \n",
            "Dựa vào Điều 59 của Luật Hàng không dân dụng Việt Nam, chức năng chính của Cảng vụ hàng không là gì?\n",
            " \n",
            " \n",
            "Chúng ta có quyền số nghiên cứu cơ sở và khoản tâm đồng với đường kinh doanh ngân hàng, công ty hoặc khu vực cần biết để mang lại tác giả, viên học, yếu tố kinh tế, vận tải, vận hành của cảng cấp hàng không dân dụng Việt Nam."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Clone llama.cpp"
      ],
      "metadata": {
        "id": "wLUfmsD1sxXl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/ggml-org/llama.cpp\n",
        "!pip -q install -r llama.cpp/requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xSEFOq22swjp",
        "outputId": "3d6718b7-7167-4f6e-c816-a3bd2ed8ea92"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'llama.cpp'...\n",
            "remote: Enumerating objects: 65617, done.\u001b[K\n",
            "remote: Counting objects: 100% (14/14), done.\u001b[K\n",
            "remote: Compressing objects: 100% (12/12), done.\u001b[K\n",
            "remote: Total 65617 (delta 4), reused 3 (delta 2), pack-reused 65603 (from 2)\u001b[K\n",
            "Receiving objects: 100% (65617/65617), 183.62 MiB | 17.88 MiB/s, done.\n",
            "Resolving deltas: 100% (47716/47716), done.\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m70.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m61.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.7/12.7 MB\u001b[0m \u001b[31m61.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m68.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.2/96.2 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.9/294.9 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m178.6/178.6 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m343.6/343.6 kB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m389.6/389.6 kB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.3/45.3 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.2/98.2 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for transformers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\n",
            "torchaudio 2.8.0+cu126 requires torch==2.8.0, but you have torch 2.6.0+cpu which is incompatible.\n",
            "jaxlib 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "jax 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "ydf 0.13.0 requires protobuf<7.0.0,>=5.29.1, but you have protobuf 4.25.8 which is incompatible.\n",
            "torchvision 0.23.0+cu126 requires torch==2.8.0, but you have torch 2.6.0+cpu which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "pytensor 2.35.1 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "grpcio-status 1.71.2 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 4.25.8 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
            "opentelemetry-proto 1.37.0 requires protobuf<7.0,>=5.0, but you have protobuf 4.25.8 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls llama.cpp"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZZID4mhEz-0U",
        "outputId": "85454d59-8bf4-42cd-b5aa-7aac4dfd0d3f"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AUTHORS\t\t\t       examples    poetry.lock\n",
            "build-xcframework.sh\t       flake.lock  pyproject.toml\n",
            "ci\t\t\t       flake.nix   pyrightconfig.json\n",
            "cmake\t\t\t       ggml\t   README.md\n",
            "CMakeLists.txt\t\t       gguf-py\t   requirements\n",
            "CMakePresets.json\t       grammars    requirements.txt\n",
            "CODEOWNERS\t\t       include\t   scripts\n",
            "common\t\t\t       LICENSE\t   SECURITY.md\n",
            "CONTRIBUTING.md\t\t       licenses    src\n",
            "convert_hf_to_gguf.py\t       Makefile    tests\n",
            "convert_hf_to_gguf_update.py   media\t   tools\n",
            "convert_llama_ggml_to_gguf.py  models\t   vendor\n",
            "convert_lora_to_gguf.py        mypy.ini\n",
            "docs\t\t\t       pocs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 \"llama.cpp/convert_hf_to_gguf.py\"  ./downloaded_model --outfile  ./model.gguf --outtype q8_0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9cpyeweWsqTh",
        "outputId": "573198f0-c49c-4e49-dceb-2ead958432c8"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:hf-to-gguf:Loading model: downloaded_model\n",
            "INFO:hf-to-gguf:Model architecture: LlamaForCausalLM\n",
            "INFO:hf-to-gguf:gguf: indexing model part 'model.safetensors'\n",
            "INFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only\n",
            "INFO:hf-to-gguf:Exporting model...\n",
            "INFO:hf-to-gguf:output.weight,               torch.float32 --> Q8_0, shape = {2048, 32003}\n",
            "INFO:hf-to-gguf:token_embd.weight,           torch.float32 --> Q8_0, shape = {2048, 32003}\n",
            "INFO:hf-to-gguf:blk.0.attn_norm.weight,      torch.float32 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.0.ffn_down.weight,       torch.float32 --> Q8_0, shape = {5632, 2048}\n",
            "INFO:hf-to-gguf:blk.0.ffn_gate.weight,       torch.float32 --> Q8_0, shape = {2048, 5632}\n",
            "INFO:hf-to-gguf:blk.0.ffn_up.weight,         torch.float32 --> Q8_0, shape = {2048, 5632}\n",
            "INFO:hf-to-gguf:blk.0.ffn_norm.weight,       torch.float32 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.0.attn_k.weight,         torch.float32 --> Q8_0, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.0.attn_output.weight,    torch.float32 --> Q8_0, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.0.attn_q.weight,         torch.float32 --> Q8_0, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.0.attn_v.weight,         torch.float32 --> Q8_0, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.1.attn_norm.weight,      torch.float32 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.1.ffn_down.weight,       torch.float32 --> Q8_0, shape = {5632, 2048}\n",
            "INFO:hf-to-gguf:blk.1.ffn_gate.weight,       torch.float32 --> Q8_0, shape = {2048, 5632}\n",
            "INFO:hf-to-gguf:blk.1.ffn_up.weight,         torch.float32 --> Q8_0, shape = {2048, 5632}\n",
            "INFO:hf-to-gguf:blk.1.ffn_norm.weight,       torch.float32 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.1.attn_k.weight,         torch.float32 --> Q8_0, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.1.attn_output.weight,    torch.float32 --> Q8_0, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.1.attn_q.weight,         torch.float32 --> Q8_0, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.1.attn_v.weight,         torch.float32 --> Q8_0, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.10.attn_norm.weight,     torch.float32 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.10.ffn_down.weight,      torch.float32 --> Q8_0, shape = {5632, 2048}\n",
            "INFO:hf-to-gguf:blk.10.ffn_gate.weight,      torch.float32 --> Q8_0, shape = {2048, 5632}\n",
            "INFO:hf-to-gguf:blk.10.ffn_up.weight,        torch.float32 --> Q8_0, shape = {2048, 5632}\n",
            "INFO:hf-to-gguf:blk.10.ffn_norm.weight,      torch.float32 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.10.attn_k.weight,        torch.float32 --> Q8_0, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.10.attn_output.weight,   torch.float32 --> Q8_0, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.10.attn_q.weight,        torch.float32 --> Q8_0, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.10.attn_v.weight,        torch.float32 --> Q8_0, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.11.attn_norm.weight,     torch.float32 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.11.ffn_down.weight,      torch.float32 --> Q8_0, shape = {5632, 2048}\n",
            "INFO:hf-to-gguf:blk.11.ffn_gate.weight,      torch.float32 --> Q8_0, shape = {2048, 5632}\n",
            "INFO:hf-to-gguf:blk.11.ffn_up.weight,        torch.float32 --> Q8_0, shape = {2048, 5632}\n",
            "INFO:hf-to-gguf:blk.11.ffn_norm.weight,      torch.float32 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.11.attn_k.weight,        torch.float32 --> Q8_0, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.11.attn_output.weight,   torch.float32 --> Q8_0, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.11.attn_q.weight,        torch.float32 --> Q8_0, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.11.attn_v.weight,        torch.float32 --> Q8_0, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.12.attn_norm.weight,     torch.float32 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.12.ffn_down.weight,      torch.float32 --> Q8_0, shape = {5632, 2048}\n",
            "INFO:hf-to-gguf:blk.12.ffn_gate.weight,      torch.float32 --> Q8_0, shape = {2048, 5632}\n",
            "INFO:hf-to-gguf:blk.12.ffn_up.weight,        torch.float32 --> Q8_0, shape = {2048, 5632}\n",
            "INFO:hf-to-gguf:blk.12.ffn_norm.weight,      torch.float32 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.12.attn_k.weight,        torch.float32 --> Q8_0, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.12.attn_output.weight,   torch.float32 --> Q8_0, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.12.attn_q.weight,        torch.float32 --> Q8_0, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.12.attn_v.weight,        torch.float32 --> Q8_0, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.13.attn_norm.weight,     torch.float32 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.13.ffn_down.weight,      torch.float32 --> Q8_0, shape = {5632, 2048}\n",
            "INFO:hf-to-gguf:blk.13.ffn_gate.weight,      torch.float32 --> Q8_0, shape = {2048, 5632}\n",
            "INFO:hf-to-gguf:blk.13.ffn_up.weight,        torch.float32 --> Q8_0, shape = {2048, 5632}\n",
            "INFO:hf-to-gguf:blk.13.ffn_norm.weight,      torch.float32 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.13.attn_k.weight,        torch.float32 --> Q8_0, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.13.attn_output.weight,   torch.float32 --> Q8_0, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.13.attn_q.weight,        torch.float32 --> Q8_0, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.13.attn_v.weight,        torch.float32 --> Q8_0, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.14.attn_norm.weight,     torch.float32 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.14.ffn_down.weight,      torch.float32 --> Q8_0, shape = {5632, 2048}\n",
            "INFO:hf-to-gguf:blk.14.ffn_gate.weight,      torch.float32 --> Q8_0, shape = {2048, 5632}\n",
            "INFO:hf-to-gguf:blk.14.ffn_up.weight,        torch.float32 --> Q8_0, shape = {2048, 5632}\n",
            "INFO:hf-to-gguf:blk.14.ffn_norm.weight,      torch.float32 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.14.attn_k.weight,        torch.float32 --> Q8_0, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.14.attn_output.weight,   torch.float32 --> Q8_0, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.14.attn_q.weight,        torch.float32 --> Q8_0, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.14.attn_v.weight,        torch.float32 --> Q8_0, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.15.attn_norm.weight,     torch.float32 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.15.ffn_down.weight,      torch.float32 --> Q8_0, shape = {5632, 2048}\n",
            "INFO:hf-to-gguf:blk.15.ffn_gate.weight,      torch.float32 --> Q8_0, shape = {2048, 5632}\n",
            "INFO:hf-to-gguf:blk.15.ffn_up.weight,        torch.float32 --> Q8_0, shape = {2048, 5632}\n",
            "INFO:hf-to-gguf:blk.15.ffn_norm.weight,      torch.float32 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.15.attn_k.weight,        torch.float32 --> Q8_0, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.15.attn_output.weight,   torch.float32 --> Q8_0, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.15.attn_q.weight,        torch.float32 --> Q8_0, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.15.attn_v.weight,        torch.float32 --> Q8_0, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.16.attn_norm.weight,     torch.float32 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.16.ffn_down.weight,      torch.float32 --> Q8_0, shape = {5632, 2048}\n",
            "INFO:hf-to-gguf:blk.16.ffn_gate.weight,      torch.float32 --> Q8_0, shape = {2048, 5632}\n",
            "INFO:hf-to-gguf:blk.16.ffn_up.weight,        torch.float32 --> Q8_0, shape = {2048, 5632}\n",
            "INFO:hf-to-gguf:blk.16.ffn_norm.weight,      torch.float32 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.16.attn_k.weight,        torch.float32 --> Q8_0, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.16.attn_output.weight,   torch.float32 --> Q8_0, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.16.attn_q.weight,        torch.float32 --> Q8_0, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.16.attn_v.weight,        torch.float32 --> Q8_0, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.17.attn_norm.weight,     torch.float32 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.17.ffn_down.weight,      torch.float32 --> Q8_0, shape = {5632, 2048}\n",
            "INFO:hf-to-gguf:blk.17.ffn_gate.weight,      torch.float32 --> Q8_0, shape = {2048, 5632}\n",
            "INFO:hf-to-gguf:blk.17.ffn_up.weight,        torch.float32 --> Q8_0, shape = {2048, 5632}\n",
            "INFO:hf-to-gguf:blk.17.ffn_norm.weight,      torch.float32 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.17.attn_k.weight,        torch.float32 --> Q8_0, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.17.attn_output.weight,   torch.float32 --> Q8_0, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.17.attn_q.weight,        torch.float32 --> Q8_0, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.17.attn_v.weight,        torch.float32 --> Q8_0, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.18.attn_norm.weight,     torch.float32 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.18.ffn_down.weight,      torch.float32 --> Q8_0, shape = {5632, 2048}\n",
            "INFO:hf-to-gguf:blk.18.ffn_gate.weight,      torch.float32 --> Q8_0, shape = {2048, 5632}\n",
            "INFO:hf-to-gguf:blk.18.ffn_up.weight,        torch.float32 --> Q8_0, shape = {2048, 5632}\n",
            "INFO:hf-to-gguf:blk.18.ffn_norm.weight,      torch.float32 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.18.attn_k.weight,        torch.float32 --> Q8_0, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.18.attn_output.weight,   torch.float32 --> Q8_0, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.18.attn_q.weight,        torch.float32 --> Q8_0, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.18.attn_v.weight,        torch.float32 --> Q8_0, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.19.attn_norm.weight,     torch.float32 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.19.ffn_down.weight,      torch.float32 --> Q8_0, shape = {5632, 2048}\n",
            "INFO:hf-to-gguf:blk.19.ffn_gate.weight,      torch.float32 --> Q8_0, shape = {2048, 5632}\n",
            "INFO:hf-to-gguf:blk.19.ffn_up.weight,        torch.float32 --> Q8_0, shape = {2048, 5632}\n",
            "INFO:hf-to-gguf:blk.19.ffn_norm.weight,      torch.float32 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.19.attn_k.weight,        torch.float32 --> Q8_0, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.19.attn_output.weight,   torch.float32 --> Q8_0, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.19.attn_q.weight,        torch.float32 --> Q8_0, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.19.attn_v.weight,        torch.float32 --> Q8_0, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.2.attn_norm.weight,      torch.float32 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.2.ffn_down.weight,       torch.float32 --> Q8_0, shape = {5632, 2048}\n",
            "INFO:hf-to-gguf:blk.2.ffn_gate.weight,       torch.float32 --> Q8_0, shape = {2048, 5632}\n",
            "INFO:hf-to-gguf:blk.2.ffn_up.weight,         torch.float32 --> Q8_0, shape = {2048, 5632}\n",
            "INFO:hf-to-gguf:blk.2.ffn_norm.weight,       torch.float32 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.2.attn_k.weight,         torch.float32 --> Q8_0, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.2.attn_output.weight,    torch.float32 --> Q8_0, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.2.attn_q.weight,         torch.float32 --> Q8_0, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.2.attn_v.weight,         torch.float32 --> Q8_0, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.20.attn_norm.weight,     torch.float32 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.20.ffn_down.weight,      torch.float32 --> Q8_0, shape = {5632, 2048}\n",
            "INFO:hf-to-gguf:blk.20.ffn_gate.weight,      torch.float32 --> Q8_0, shape = {2048, 5632}\n",
            "INFO:hf-to-gguf:blk.20.ffn_up.weight,        torch.float32 --> Q8_0, shape = {2048, 5632}\n",
            "INFO:hf-to-gguf:blk.20.ffn_norm.weight,      torch.float32 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.20.attn_k.weight,        torch.float32 --> Q8_0, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.20.attn_output.weight,   torch.float32 --> Q8_0, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.20.attn_q.weight,        torch.float32 --> Q8_0, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.20.attn_v.weight,        torch.float32 --> Q8_0, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.21.attn_norm.weight,     torch.float32 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.21.ffn_down.weight,      torch.float32 --> Q8_0, shape = {5632, 2048}\n",
            "INFO:hf-to-gguf:blk.21.ffn_gate.weight,      torch.float32 --> Q8_0, shape = {2048, 5632}\n",
            "INFO:hf-to-gguf:blk.21.ffn_up.weight,        torch.float32 --> Q8_0, shape = {2048, 5632}\n",
            "INFO:hf-to-gguf:blk.21.ffn_norm.weight,      torch.float32 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.21.attn_k.weight,        torch.float32 --> Q8_0, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.21.attn_output.weight,   torch.float32 --> Q8_0, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.21.attn_q.weight,        torch.float32 --> Q8_0, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.21.attn_v.weight,        torch.float32 --> Q8_0, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.3.attn_norm.weight,      torch.float32 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.3.ffn_down.weight,       torch.float32 --> Q8_0, shape = {5632, 2048}\n",
            "INFO:hf-to-gguf:blk.3.ffn_gate.weight,       torch.float32 --> Q8_0, shape = {2048, 5632}\n",
            "INFO:hf-to-gguf:blk.3.ffn_up.weight,         torch.float32 --> Q8_0, shape = {2048, 5632}\n",
            "INFO:hf-to-gguf:blk.3.ffn_norm.weight,       torch.float32 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.3.attn_k.weight,         torch.float32 --> Q8_0, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.3.attn_output.weight,    torch.float32 --> Q8_0, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.3.attn_q.weight,         torch.float32 --> Q8_0, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.3.attn_v.weight,         torch.float32 --> Q8_0, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.4.attn_norm.weight,      torch.float32 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.4.ffn_down.weight,       torch.float32 --> Q8_0, shape = {5632, 2048}\n",
            "INFO:hf-to-gguf:blk.4.ffn_gate.weight,       torch.float32 --> Q8_0, shape = {2048, 5632}\n",
            "INFO:hf-to-gguf:blk.4.ffn_up.weight,         torch.float32 --> Q8_0, shape = {2048, 5632}\n",
            "INFO:hf-to-gguf:blk.4.ffn_norm.weight,       torch.float32 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.4.attn_k.weight,         torch.float32 --> Q8_0, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.4.attn_output.weight,    torch.float32 --> Q8_0, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.4.attn_q.weight,         torch.float32 --> Q8_0, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.4.attn_v.weight,         torch.float32 --> Q8_0, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.5.attn_norm.weight,      torch.float32 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.5.ffn_down.weight,       torch.float32 --> Q8_0, shape = {5632, 2048}\n",
            "INFO:hf-to-gguf:blk.5.ffn_gate.weight,       torch.float32 --> Q8_0, shape = {2048, 5632}\n",
            "INFO:hf-to-gguf:blk.5.ffn_up.weight,         torch.float32 --> Q8_0, shape = {2048, 5632}\n",
            "INFO:hf-to-gguf:blk.5.ffn_norm.weight,       torch.float32 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.5.attn_k.weight,         torch.float32 --> Q8_0, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.5.attn_output.weight,    torch.float32 --> Q8_0, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.5.attn_q.weight,         torch.float32 --> Q8_0, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.5.attn_v.weight,         torch.float32 --> Q8_0, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.6.attn_norm.weight,      torch.float32 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.6.ffn_down.weight,       torch.float32 --> Q8_0, shape = {5632, 2048}\n",
            "INFO:hf-to-gguf:blk.6.ffn_gate.weight,       torch.float32 --> Q8_0, shape = {2048, 5632}\n",
            "INFO:hf-to-gguf:blk.6.ffn_up.weight,         torch.float32 --> Q8_0, shape = {2048, 5632}\n",
            "INFO:hf-to-gguf:blk.6.ffn_norm.weight,       torch.float32 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.6.attn_k.weight,         torch.float32 --> Q8_0, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.6.attn_output.weight,    torch.float32 --> Q8_0, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.6.attn_q.weight,         torch.float32 --> Q8_0, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.6.attn_v.weight,         torch.float32 --> Q8_0, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.7.attn_norm.weight,      torch.float32 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.7.ffn_down.weight,       torch.float32 --> Q8_0, shape = {5632, 2048}\n",
            "INFO:hf-to-gguf:blk.7.ffn_gate.weight,       torch.float32 --> Q8_0, shape = {2048, 5632}\n",
            "INFO:hf-to-gguf:blk.7.ffn_up.weight,         torch.float32 --> Q8_0, shape = {2048, 5632}\n",
            "INFO:hf-to-gguf:blk.7.ffn_norm.weight,       torch.float32 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.7.attn_k.weight,         torch.float32 --> Q8_0, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.7.attn_output.weight,    torch.float32 --> Q8_0, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.7.attn_q.weight,         torch.float32 --> Q8_0, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.7.attn_v.weight,         torch.float32 --> Q8_0, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.8.attn_norm.weight,      torch.float32 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.8.ffn_down.weight,       torch.float32 --> Q8_0, shape = {5632, 2048}\n",
            "INFO:hf-to-gguf:blk.8.ffn_gate.weight,       torch.float32 --> Q8_0, shape = {2048, 5632}\n",
            "INFO:hf-to-gguf:blk.8.ffn_up.weight,         torch.float32 --> Q8_0, shape = {2048, 5632}\n",
            "INFO:hf-to-gguf:blk.8.ffn_norm.weight,       torch.float32 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.8.attn_k.weight,         torch.float32 --> Q8_0, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.8.attn_output.weight,    torch.float32 --> Q8_0, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.8.attn_q.weight,         torch.float32 --> Q8_0, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.8.attn_v.weight,         torch.float32 --> Q8_0, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.9.attn_norm.weight,      torch.float32 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.9.ffn_down.weight,       torch.float32 --> Q8_0, shape = {5632, 2048}\n",
            "INFO:hf-to-gguf:blk.9.ffn_gate.weight,       torch.float32 --> Q8_0, shape = {2048, 5632}\n",
            "INFO:hf-to-gguf:blk.9.ffn_up.weight,         torch.float32 --> Q8_0, shape = {2048, 5632}\n",
            "INFO:hf-to-gguf:blk.9.ffn_norm.weight,       torch.float32 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.9.attn_k.weight,         torch.float32 --> Q8_0, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.9.attn_output.weight,    torch.float32 --> Q8_0, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.9.attn_q.weight,         torch.float32 --> Q8_0, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.9.attn_v.weight,         torch.float32 --> Q8_0, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:output_norm.weight,          torch.float32 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:Set meta model\n",
            "INFO:hf-to-gguf:Set model parameters\n",
            "INFO:hf-to-gguf:gguf: context length = 2048\n",
            "INFO:hf-to-gguf:gguf: embedding length = 2048\n",
            "INFO:hf-to-gguf:gguf: feed forward length = 5632\n",
            "INFO:hf-to-gguf:gguf: head count = 32\n",
            "INFO:hf-to-gguf:gguf: key-value head count = 4\n",
            "INFO:hf-to-gguf:gguf: rope theta = 10000.0\n",
            "INFO:hf-to-gguf:gguf: rms norm epsilon = 1e-05\n",
            "INFO:hf-to-gguf:gguf: file type = 7\n",
            "INFO:hf-to-gguf:Set model quantization version\n",
            "INFO:hf-to-gguf:Set model tokenizer\n",
            "WARNING:gguf.vocab:Unknown separator token '<s>' in TemplateProcessing<pair>\n",
            "INFO:gguf.vocab:Setting special token type bos to 1\n",
            "INFO:gguf.vocab:Setting special token type eos to 2\n",
            "INFO:gguf.vocab:Setting special token type unk to 0\n",
            "INFO:gguf.vocab:Setting special token type pad to 2\n",
            "INFO:gguf.vocab:Setting add_bos_token to True\n",
            "INFO:gguf.vocab:Setting add_sep_token to False\n",
            "INFO:gguf.vocab:Setting add_eos_token to False\n",
            "INFO:gguf.vocab:Setting chat_template to {% for message in messages %}\n",
            "{% if message['role'] == 'system' %}\n",
            "<|system|>\n",
            "{{ message['content'].strip() }}\n",
            "{{ eos_token }}\n",
            "{% elif message['role'] == 'user' %}\n",
            "<|user|>\n",
            "{{ message['content'].strip() }}\n",
            "{{ eos_token }}\n",
            "{% elif message['role'] == 'assistant' %}\n",
            "<|assistant|>\n",
            "{{ message['content'].strip() }}\n",
            "{{ eos_token }}\n",
            "{% endif %}\n",
            "{% endfor %}\n",
            "{% if add_generation_prompt %}\n",
            "<|assistant|>\n",
            "{% endif %}\n",
            "INFO:gguf.gguf_writer:Writing the following files:\n",
            "INFO:gguf.gguf_writer:model.gguf: n_tensors = 201, total_size = 1.2G\n",
            "Writing: 100% 1.17G/1.17G [00:29<00:00, 39.7Mbyte/s]\n",
            "INFO:hf-to-gguf:Model successfully exported to model.gguf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install llama-cpp-python"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YrGl1EVf0wsW",
        "outputId": "43c8295d-8ea9-4edf-c7a6-54c316a68703"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.7/50.7 MB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m542.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inference"
      ],
      "metadata": {
        "id": "P_hVz1lLc-5b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_cpp import Llama\n",
        "\n",
        "llm = Llama(model_path=\"./model.gguf\",  verbose=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TMO-boDUwDc3",
        "outputId": "c6e57509-aa4c-49e9-fcb3-abdbed8c1d45"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_context: n_ctx_per_seq (512) < n_ctx_train (2048) -- the full capacity of the model will not be utilized\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = [\n",
        "    {\"role\": \"system\", \"content\": \"Bạn là một trợ lý luật pháp Việt Nam thông minh, luôn trả lời bằng tiếng Việt chuẩn và dễ hiểu.\"},\n",
        "    {\"role\": \"user\", \"content\": question}\n",
        "]\n",
        "start = time.time()\n",
        "\n",
        "generation_options = {\n",
        "    \"max_tokens\": 384,\n",
        "    \"temperature\": 0.7,\n",
        "    \"top_p\": 0.9,\n",
        "    \"top_k\": 40,\n",
        "    \"stop\": None,\n",
        "}\n",
        "\n",
        "response = llm.create_chat_completion(messages=prompt, **generation_options)\n",
        "durations = time.time() - start\n",
        "\n",
        "print(response['choices'][0]['message']['content'])\n",
        "print(f\"Time: {durations:.3f} giây\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_jAqmamn0yAB",
        "outputId": "8f9ec4a0-f9c4-4a6f-9c76-06eff7c81492"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cảng vụ hàng không có được chính của Cảng, chức năng của Cảng vụ hàng không là gì.\n",
            "Time: 21.154 giây\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Streaming"
      ],
      "metadata": {
        "id": "CGuGyHlPdBjK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = [\n",
        "    {\"role\": \"system\", \"content\": \"Bạn là một trợ lý luật pháp Việt Nam thông minh, luôn trả lời bằng tiếng Việt chuẩn và dễ hiểu.\"},\n",
        "    {\"role\": \"user\", \"content\": question}\n",
        "]\n",
        "\n",
        "generation_options = {\n",
        "    \"max_tokens\": 256,\n",
        "    \"temperature\": 0.7,\n",
        "    \"top_p\": 0.9,\n",
        "    \"top_k\": 40,\n",
        "    \"stop\": None,\n",
        "}\n",
        "\n",
        "for chunk in llm.create_chat_completion(messages=prompt, stream=True, **generation_options):\n",
        "    choice = chunk['choices'][0]['delta']\n",
        "    if 'role' in choice:\n",
        "        print(f\"\\n[{choice['role']}]: \", end='', flush=True)\n",
        "    if 'content' in choice:\n",
        "        print(choice['content'], end='', flush=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0luFJLb98OSy",
        "outputId": "2da78bf6-0f55-426b-9ce0-65d6fe96fc38"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[assistant]: Dựa vào Điều 59 của Luật Hàng không dân dụng Việt Nam, chức năng chính của Cảng vụ hàng không là gì là một thuật ngữ của Cảng vụ hàng không, làm tốt khi Cảng vụ hàng không có cơ sở sửa lọc, có cơ hội kết nối với cơ quan tư vấn và tỉnh thành hoặc khác, có cơ hội bảo trì và cơ hội đăng ký. Chức năng chính của Cảng vụ hàng không là gì là cơ cấu chính của Cảng vụ hàng không, làm tốt khi Cảng v"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mlflow registry model"
      ],
      "metadata": {
        "id": "TqNomNMzt0ed"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with mlflow.start_run() as run:\n",
        "\n",
        "    s3_client = boto3.client(\n",
        "        \"s3\",\n",
        "        aws_access_key_id=os.environ[\"AWS_ACCESS_KEY_ID\"],\n",
        "        aws_secret_access_key=os.environ[\"AWS_SECRET_ACCESS_KEY\"],\n",
        "        region_name=os.environ[\"AWS_DEFAULT_REGION\"]\n",
        "    )\n",
        "\n",
        "    bucket = \"mlflow-artifacts-monitor\"\n",
        "    s3_prefix = f\"models/health-llm/{run.info.run_id}\"\n",
        "\n",
        "\n",
        "    model_path = \"gguf_model.gguf\"\n",
        "    key = f\"{s3_prefix}/{model_path}\"\n",
        "    s3_client.upload_file(model_path, bucket, key)\n",
        "\n",
        "    model_uri = f\"s3://{bucket}/{s3_prefix}\"\n",
        "\n",
        "    REGISTERED_MODEL_NAME = \"health-llm-gguf\"\n",
        "\n",
        "    result = mlflow.register_model(\n",
        "        model_uri=model_uri,\n",
        "        name=REGISTERED_MODEL_NAME\n",
        "    )\n",
        "\n",
        "    client = MlflowClient()\n",
        "\n",
        "    client.set_registered_model_tag(\n",
        "        name=REGISTERED_MODEL_NAME, key=\"use_case\", value=\"patient_service\"\n",
        "    )\n",
        "\n",
        "    client.update_registered_model(\n",
        "        name=REGISTERED_MODEL_NAME,\n",
        "        description=\"A health-specific chatbot about daily Vietnamese sickness questions\"\n",
        "    )\n",
        "\n",
        "    client.set_model_version_tag(\n",
        "        name=REGISTERED_MODEL_NAME,\n",
        "        version=result.version,\n",
        "        key=\"validation_status\",\n",
        "        value=\"testing\",\n",
        "    )\n",
        "\n",
        "    client.set_registered_model_alias(\n",
        "        name=REGISTERED_MODEL_NAME,\n",
        "        alias=\"champion\",\n",
        "        version=result.version,\n",
        "    )\n",
        "\n",
        "    print(f\"Model registered successfully: version {result.version}\")\n",
        "    print(f\"S3 path: {model_uri}\")\n",
        "    print(f\"MLflow tracking: {run.info.run_id}\")"
      ],
      "metadata": {
        "id": "7ME3V2X68OmW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99cebe5f-b470-4bf2-a5c9-cc5d54ba22de"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Successfully registered model 'health-llm-gguf'.\n",
            "2025/10/25 11:02:17 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: health-llm-gguf, version 1\n",
            "Created version '1' of model 'health-llm-gguf'.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model registered successfully: version 1\n",
            "S3 path: s3://mlflow-artifacts-monitor/models/health-llm/3edc1eb810bb40728fb7962a42d61530\n",
            "MLflow tracking: 3edc1eb810bb40728fb7962a42d61530\n",
            "🏃 View run nervous-wren-355 at: https://victoria-communicable-sometimes.ngrok-free.dev/#/experiments/1/runs/3edc1eb810bb40728fb7962a42d61530\n",
            "🧪 View experiment at: https://victoria-communicable-sometimes.ngrok-free.dev/#/experiments/1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lAMW4EdWvjn1"
      },
      "execution_count": 21,
      "outputs": []
    }
  ]
}